# Neural_Network_Charity_Analysis

## Overview of the Project
From Alphabet Soup’s business team, we received a CSV containing more than 34,000 organizations that have received funding from Alphabet Soup over the years. Therefore, using deep-learning neural networks with the TensorFlow platform in Python, to analyze and predict whether applicants will be successful if funded by Alphabet Soup.

## Approaches
We use the following methods for the analysis:
- preprocessing the data for the neural network model,
- compile, train and evaluate the model with test data,
- optimize the model to imporve prediction accuracy.

## Resources
Datasets [Charity Data](https://github.com/ShiraliObul/Neural_Network_Charity_Analysis/blob/main/Resources/charity_data.csv)
Tools: Python, Pandas, Numpy, Scikit, TensorFlow, Jupyter lab

## Results 
### Processing the dataset in order to compile, train, and evaluate the neural network model
There are a number of columns that capture metadata about each organization, such as the following:
- EIN and NAME—Identification columns
- APPLICATION_TYPE—Alphabet Soup application type
- AFFILIATION—Affiliated sector of industry
- CLASSIFICATION—Government organization classification
- USE_CASE—Use case for funding
- ORGANIZATION—Organization type
- STATUS—Active status
- INCOME_AMT—Income classification
- SPECIAL_CONSIDERATIONS—Special consideration for application
- ASK_AMT—Funding amount requested
- IS_SUCCESSFUL—Was the money used effectively

